services:
  inference-job:
    build:
      context: ../
      dockerfile: ops/docker/Dockerfile.inference
    env_file:
      - ../config/secrets.env
      - inference.env
    environment:
      - SETTINGS_PATH=/app/config/settings.yaml
      - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI:-http://mlflow:5000}
    volumes:
      - ../storage/data:/app/storage/data
      - ../storage/artifacts:/app/storage/artifacts
    command: >-
      python -m pipelines.score_baseline
    depends_on:
      mlflow:
        condition: service_started

  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    env_file:
      - mlflow.env
    command: >-
      mlflow server
        --backend-store-uri $${MLFLOW_BACKEND_STORE_URI}
        --default-artifact-root $${MLFLOW_DEFAULT_ARTIFACT_ROOT}
        --host 0.0.0.0
        --port 5000

volumes: {}
